{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"350\" style=\"float:right\" \n",
    "     src=\"https://cdn.i.haymarketmedia.asia/?n=campaign-asia%2Fcontent%2FcroppedF1logo.png&h=570&w=855&q=100&v=20170226&c=1\" />\n",
    "\n",
    "# Formula 1 Analysis Group D\n",
    "\n",
    "\n",
    "Beltran Ramirez\\\n",
    "Cassady Cook\\\n",
    "Felix Massenet\\\n",
    "Julius Prestin\\\n",
    "Enrique Recke\\\n",
    "Raphael Widmer\\\n",
    "Victoria Roguet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "\n",
    "* [1. PySpark environment setup](#1)\n",
    "  * [1.1 Search for Spark Installation](#1.1)  \n",
    "  * [1.2 Create SparkSession](#1.2)\n",
    "* [2. Data source and Spark data abstraction (DataFrame) setup](#2)\n",
    "* [3. Data sets Meta Analysis](#3)\n",
    "* [4. Tweets Analysis](#4)\n",
    "  * [4.1 Check Twitter Files](#4.1)\n",
    "  * [4.2 Create the DataFrame](#4.2)\n",
    "  * [4.3 General analysis of the Tweets](#4.3)\n",
    "* [5. GraphFrames](#5)\n",
    "  * [5.1 Hashtag Graph](#5.1)\n",
    "  * [5.2 User Mentions Graph](#5.2)\n",
    "* [6. Machine Learning](#6)\n",
    "  * [6.1 Feature Transformation](#6.1)\n",
    "  * [6.2 Model Training & Selection](#6.2)\n",
    "  * [6.3 Model Predictions](#6.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. PySpark environment setup\n",
    "\n",
    "To setup our PySpark environment we:\n",
    "1. Search for Spark Installation\n",
    "2. Create SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change pandas max column width property in order to improve data displaying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Create SparkSession\n",
    "\n",
    "By setting this environment variable we can include extra libraries in our Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access catalogue in Hive\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /opt/hive3/lib/hive-hcatalog-core-3.1.2.jar pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\" --jars /opt/hive3/lib/hive-hcatalog-core-3.1.2.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing always is to create the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Twitter Analytics - DataFrames\")\n",
    "    .config(\"spark.sql.warehouse.dir\",\"hdfs://localhost:9000/warehouse\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Data source and Spark data abstraction (DataFrame) setup\n",
    "\n",
    "\n",
    "- In order to get an overview of the industry, we imported one merged file with data from 2014 to 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to infer the schema of the datasets as there is too many columns and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F1_main = spark.read.option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .csv(\"main_f1.csv\")\\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Data sets Meta Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "F1_main.printSchema()\n",
    "display(Markdown(\"This F1 DataFrame has **%d rows**.\" % F1_main.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are importing all the libraries for the further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# 4. Tweets Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "## 4.1 Check Twitter Files\n",
    "\n",
    "We checked if our ingested data was saved properly in the following HDFS directory: \n",
    "\n",
    "`http://localhost:50070/explorer.html#/datalake/raw/twitter/F1MDA2/*/*/*/*`\n",
    "\n",
    "Notice that we use `*` since we have stored the data using a date structure of `year/month/day/files`.\n",
    "\n",
    "Tweet Object Model: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "## 4.2 Create the DataFrame\n",
    "\n",
    "Here we are creating the DataFrames with the stored tweets in order to do some metadata and data inspection to answer the business questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = (spark.read\n",
    "               .json(\"hdfs://localhost:9000//datalake/raw/twitter/F1MDA2/*/*/*/*\"))\n",
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.select(\"entities\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "## 4.3 General Analysis of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of Tweets**<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of distinct users**<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.select(\"user.id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total number of users with geolocation enabled**<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.where(\"user.geo_enabled=true\").select(\"user.id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 user locations**<br/>\n",
    "\n",
    "- Here we tried to find out user location for each tweet but since there are many accounts that disabled geo_location, we filtered out the empty locations to view only those with a location assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = (tweets\n",
    "          .where(tweets.user.location != \"\")\n",
    "          .groupBy(\"user.location\")\n",
    "          .agg(F.count(\"*\").alias(\"tweets\"))\n",
    "          .orderBy(F.desc(\"tweets\")))\n",
    "\n",
    "df_location.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 users with more mentions**<br/>\n",
    "- Since Ferrari won the first race of the 2022 season, and we continued the ingestion through this first race weekend, we see that Ferrari was mentioned the most times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_with_more_mentions = (tweets\n",
    "          .select(F.explode(\"entities.user_mentions.screen_name\").alias(\"user\"))\n",
    "          .where((F.lower('user') != 'f1'))\n",
    "          .groupBy(\"user\")\n",
    "          .agg(F.count(\"*\").alias(\"mentions\"))\n",
    "          .orderBy(F.desc(\"mentions\"))\n",
    "          .limit(10))\n",
    "df_users_with_more_mentions.toPandas().plot.barh(x = 'user', y = 'mentions', title = 'Mentions per User', figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tweets per day**<br/>\n",
    "\n",
    "We wanted to count the number of tweets per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.select((F.split('created_at', \" \")[2]).alias('Day'))\\\n",
    "                .groupBy('Day')\\\n",
    "                .agg(F.count(\"*\").alias(\"total\"))\\\n",
    "                .orderBy(F.asc(\"Day\"))\\\n",
    "                .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 more popular hashtags**<br/>\n",
    "\n",
    "- We wanted to filter out the **#F1** hashtag, since all the tweets we stored used this hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (tweets\n",
    "      .select(F.explode(\"entities.hashtags.text\").alias(\"hashtag\"))\n",
    "      .where((F.upper('hashtag') != 'F1'))\n",
    "      .groupBy(F.upper(\"hashtag\"))\n",
    "      .agg(F.count(\"*\").alias(\"total\"))\n",
    "      .orderBy(F.desc(\"total\"))\n",
    "      .limit(10))\n",
    "      \n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. GraphFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "## 5.1 Hashtag Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to create a function that returns a new dataframe with the hashtags. <br/> \n",
    "- We are just considering the tweets that has more than 3 hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "def elements_by_tweets(df, entity):\n",
    "    return (tweets.where(size(f\"entities.{entity}.text\") > 3)\n",
    "                .select(\"id\",\"text\",col(f\"entities.{entity}.text\").alias(\"elements\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find all the tweets with at least two **hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_by_tweet = elements_by_tweets(tweets,\"hashtags\")\n",
    "hashtags_by_tweet.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tweets have repeated entities and some of them are mixing lower and upper case. Let's fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"array<string>\")\n",
    "def upper_case(a):\n",
    "    if not a:\n",
    "        return a\n",
    "    return list(map(lambda x: x.upper(), a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_by_tweets_fixed = hashtags_by_tweet.withColumn(\"hashtags\",array_sort(array_distinct(upper_case(\"elements\"))))\n",
    "hashtags_by_tweets_fixed.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create all possible elements permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "@udf(\"array<array<string>>\")\n",
    "def combine(a):\n",
    "    return list(permutations(a, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_pair_by_tweet = hashtags_by_tweets_fixed.select(\"id\",combine(\"hashtags\").alias(\"permutations\"))\n",
    "hashtag_pair_by_tweet.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explode the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hashtags = (hashtag_pair_by_tweet\n",
    "                     .select(explode(\"permutations\").alias(\"pair\"))\n",
    "                     .select(col(\"pair\")[0].alias(\"hashtag1\"),\n",
    "                             col(\"pair\")[1].alias(\"hashtag2\")))\n",
    "                                                       \n",
    "hashtags.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the hashtags dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vertices = (hashtags.select(\"hashtag1\").union(hashtags.select(\"hashtag2\"))\n",
    "           .distinct()           \n",
    "           .withColumnRenamed(\"hashtag1\",\"id\"))\n",
    "        \n",
    "h_edges = (hashtags\n",
    "            .withColumnRenamed(\"hashtag1\",\"src\")\n",
    "            .withColumnRenamed(\"hashtag2\",\"dst\")\n",
    "            .groupBy(\"src\",\"dst\")\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"occurrences\")\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vertices.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_edges.limit(5).orderBy(desc(\"occurrences\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the GraphFrame\n",
    "\n",
    "We are going to model our graph in the following way:<br/>\n",
    "**vertices** : hashtags <br/>\n",
    "**edges** : hashtags pairs aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the extensions that need to be installed for the graphs\n",
    "#!pip install graphframes\n",
    "#!pip3 install ipycytoscape\n",
    "#!jupyter nbextension enable -- ipycytoscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "hashtag_graph = GraphFrame(h_vertices, h_edges)\n",
    "hashtag_graph.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### which are the top 10 most common hashtags pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_graph.edges.orderBy(desc(\"occurrences\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### which are the most relevant hashtags?\n",
    "We are going to apply the Page Ranks algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_rank = hashtag_graph.pageRank(resetProbability=0.15, maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtag_rank.vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_rank.edges.limit(5).orderBy(desc(\"weight\")).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results\n",
    "Let's visualize the results.<br/>\n",
    "I'm going to use **ipycytoscape** library \n",
    "https://js.cytoscape.org/\n",
    "```sh\n",
    "pip3 install ipycytoscape\n",
    "jupyter nbextension enable --py ipycytoscape\n",
    "```\n",
    "You need to restart your VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtags Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_rank.vertices.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_rank.edges.toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hashtags_json = {}\n",
    "\n",
    "vs = []\n",
    "for v in hashtag_rank.vertices.collect():\n",
    "    data = {}\n",
    "    data['id']=v.id\n",
    "    data['label']=v.id\n",
    "    data['pagerank']=v.pagerank\n",
    "    data['color']=\"#\"+''.join([random.choice('ABCDEF0123456789') for i in range(6)])\n",
    "    element = {}\n",
    "    element['data']=data\n",
    "    vs.append(element)\n",
    "hashtags_json['nodes'] = vs\n",
    "\n",
    "es = []\n",
    "for e in hashtag_rank.edges.collect():\n",
    "    data = {}\n",
    "    data['source']=e.src\n",
    "    data['target']=e.dst\n",
    "    data['occurrences']=e.occurrences\n",
    "    data['weight']=e.weight    \n",
    "    element = {}\n",
    "    element['data']=data\n",
    "    es.append(element)\n",
    "hashtags_json['edges'] = es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipycytoscape\n",
    "\n",
    "cytoscapeobj = ipycytoscape.CytoscapeWidget()\n",
    "#adds the data\n",
    "cytoscapeobj.graph.add_graph_from_json(hashtags_json)\n",
    "#styles the nodes and egdes\n",
    "cytoscapeobj.set_style([\n",
    "            {\n",
    "                'selector': 'node',\n",
    "                'css': {'label': 'data(label)', 'background-color':'data(color)'}\n",
    "            },                        \n",
    "            {\n",
    "                'selector': 'node[id=\"F1\"]',\n",
    "                'css': {'background-color': 'orange'}\n",
    "            },            \n",
    "            {\n",
    "                'selector': 'edge',\n",
    "                'css': {\"curve-style\":\"haystack\",\"haystack-radius\":0,\"width\":2,\"opacity\":0.5,\"line-color\":\"#a8eae5\"}\n",
    "            }    \n",
    "            ])\n",
    "\n",
    "cytoscapeobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Export\n",
    "\n",
    "If we need to export the graph we can use the datasources API to create the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vertices.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"hdfs://localhost:9000/export/hashtag_vertices/\")\n",
    "h_edges.coalesce(1).withColumnRenamed(\"src\",\"Source\")\\\n",
    "                   .withColumnRenamed(\"dst\",\"Target\")\\\n",
    "                   .write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"hdfs://localhost:9000/export/hashtag_edges/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this files with tools like <a href='https://gephi.org/'>Gephi</a> to create stunning graph visualizations like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to model our graph in the following way: <br/>\n",
    "**vertices** : tweet location <br/>\n",
    "**edges** : twitter account location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "## 5.2 User Mention Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are going to create a function that returns a new dataframe with the user mentions. <br/> \n",
    "- We are just considering the tweets that has more than 4 user mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_mentions(df, entity):\n",
    "    return (tweets.where((size(f\"entities.{entity}\") > 4))\n",
    "                 .select(\"id\",\"text\",col(f\"entities.{entity}.name\").alias(\"users_mentions\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = user_mentions(tweets, 'user_mentions')\n",
    "user.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "@udf(\"array<array<string>>\")\n",
    "def combine(a):\n",
    "    return list(permutations(a, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to show all the paired permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_users = user.select('id', combine('users_mentions').alias('permutations'))\n",
    "pair_users.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a DataFrame with all the exploded permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = (pair_users\n",
    "         .select(explode(\"permutations\").alias('pair'))\n",
    "        .select(col(\"pair\")[0].alias('user_1'), col('pair')[1].alias('user_2')))\n",
    "\n",
    "users.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up the vertices and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_vertices = (users.select('user_1').union(users.select('user_2'))\n",
    "             .distinct()\n",
    "             .withColumnRenamed('user_1', 'id'))\n",
    "\n",
    "u_edges = (users\n",
    "          .withColumnRenamed('user_1', 'src')\n",
    "          .withColumnRenamed('user_2', 'dst')\n",
    "          .groupBy('src', 'dst')\n",
    "          .agg(\n",
    "              count('*').alias('occurrences')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_graph = GraphFrame(u_vertices, u_edges)\n",
    "users_graph.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we looked at the top 30 connected users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_graph.edges.orderBy(desc('occurrences')).limit(30).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_rank = users_graph.pageRank(resetProbability = 0.15, maxIter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_rank.vertices.orderBy(desc('pagerank')).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = []\n",
    "for v in users_rank.vertices.collect():\n",
    "#for v in hv.collect():\n",
    "    data = {}\n",
    "    data['id']=v.id\n",
    "    data['label']=v.id\n",
    "    data['pagerank']=v.pagerank\n",
    "    data['color']=\"#\"+''.join([random.choice('ABCDEF0123456789') for i in range(6)])\n",
    "    element = {}\n",
    "    element['data']=data\n",
    "    vs.append(element)\n",
    "hashtags_json['nodes'] = vs\n",
    "\n",
    "es = []\n",
    "for e in users_rank.edges.collect():\n",
    "#for e in he():\n",
    "    data = {}\n",
    "    data['source']=e.src\n",
    "    data['target']=e.dst\n",
    "    data['occurrences']=e.occurrences\n",
    "    data['weight']=e.weight    \n",
    "    element = {}\n",
    "    element['data']=data\n",
    "    es.append(element)\n",
    "hashtags_json['edges'] = es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cytoscapeobj = ipycytoscape.CytoscapeWidget()\n",
    "#adds the data\n",
    "cytoscapeobj.graph.add_graph_from_json(hashtags_json)\n",
    "#styles the nodes and egdes\n",
    "cytoscapeobj.set_style([\n",
    "            {\n",
    "                'selector': 'node',\n",
    "                'css': {'label': 'data(label)', 'background-color':'data(color)'}\n",
    "            },                        \n",
    "            {\n",
    "                'selector': 'node[id=\"Formula1\"]',\n",
    "                'css': {'background-color': 'orange'}\n",
    "            },            \n",
    "            {\n",
    "                'selector': 'edge',\n",
    "                'css': {\"curve-style\":\"haystack\",\"haystack-radius\":0,\"width\":2,\"opacity\":0.5,\"line-color\":\"#a8eae5\"}\n",
    "            }    \n",
    "            ])\n",
    "\n",
    "cytoscapeobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Export User Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_vertices.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"hdfs://localhost:9000/export/users_vertices/\")\n",
    "u_edges.coalesce(1).withColumnRenamed(\"src\",\"Source\")\\\n",
    "                   .withColumnRenamed(\"dst\",\"Target\")\\\n",
    "                   .write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"hdfs://localhost:9000/export/users_edges/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the F1_main data we are now creating a model to predict the points for each driver at the end of the 2021 season. <br/>\n",
    "We will validate our model with the real 2021 data since the season has already ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import *\n",
    "from pyspark.sql.functions import *\n",
    "import copy\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_main.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_main = F1_main.withColumn('finish_position', F1_main.finish_position.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_main = F1_main.withColumn('winner', \\\n",
    "                                   when(F1_main.finish_position > 1.0, 0).when(F1_main.finish_position == 1.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_main.limit(1).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we label our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = 'points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "## 6.1 Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate into categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [field for (field, dataType)\\\n",
    "                   in F1_main.dtypes\\\n",
    "                   if (((dataType == \"string\") | (dataType == 'int')\\\n",
    "                       | (dataType == 'double')) \\\n",
    "                       & (field in ('direction', 'country', 'locality', 'type', 'season', 'round', 'qual_position', 'grid', 'race_name')))]\n",
    "\n",
    "\n",
    "\n",
    "num_cols = [field for (field, dataType) in F1_main.dtypes\\\n",
    "               if (((dataType == \"double\") | (dataType == 'int'))\\\n",
    "                   & (field in ('q_mean', 'q_best', 'q_worst', 'length', 'ageDuringRace', 'finish_position', 'filled_splits')))]\n",
    "\n",
    "\n",
    "print (f\"categorical columns: {cat_cols}\")\n",
    "print (f\"numerical columns: {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol = c, outputCol=\"{0}_indexed\".format(c)) for c in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "outputCol=[\"{0}_indexed\".format(c) for c in cat_cols]\n",
    "outputCol_enc = [\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers ]\n",
    "stringIndexer = StringIndexer(inputCols=cat_cols, outputCols=outputCol, handleInvalid=\"skip\")\n",
    "\n",
    "oheEncoder = OneHotEncoder(inputCols=outputCol,outputCols=outputCol_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a vector for the encoded categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_col_assembler = [e for e in outputCol_enc]\n",
    "\n",
    "cat_col_assembler = VectorAssembler(inputCols = in_col_assembler, outputCol = 'categorical')\n",
    "\n",
    "in_col_assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pipeline for the categorical columns and fit it to the F1 main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cat = Pipeline(stages = [stringIndexer, oheEncoder, cat_col_assembler])\n",
    "F1_main = pipeline_cat.fit(F1_main).transform(F1_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale the numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "columns_to_scale = ['q_best', 'q_worst', 'q_mean', 'ageDuringRace', 'length', 'filled_splits', 'finish_position']\n",
    "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(F1_main)\n",
    "#scaledData = scalerModel.transform(train)\n",
    "\n",
    "F1_main = scalerModel.transform(F1_main)\n",
    "F1_main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_num_assembler = ['q_best_scaled',\n",
    " 'q_worst_scaled',\n",
    " 'q_mean_scaled',\n",
    " 'ageDuringRace_scaled',\n",
    " 'length_scaled',\n",
    " 'filled_splits_scaled',\n",
    " 'finish_position_scaled']\n",
    "\n",
    "in_num_assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a vector for the scaled numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerNum = VectorAssembler(inputCols = in_num_assembler, outputCol = \"num\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a Pipeline for the numerical columns and fit it to the F1 main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineNum = Pipeline(stages = [assemblerNum])\n",
    "F1_main = pipelineNum.fit(F1_main).transform(F1_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create an additional vector assembler for both categorical and numerical columns that outputs are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"categorical\", \"num\"], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "## 6.2 Model Training & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the Dataset to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = F1_main.filter(F1_main.season < 2021)\n",
    "test = F1_main.filter(F1_main.season == 2021)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test different algorithms and we will choose the one with the best evaluator score for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LinearRegression(labelCol = 'points', featuresCol = 'features')\n",
    "lr = LogisticRegression(labelCol=\"points\", featuresCol=\"features\")\n",
    "nb = NaiveBayes(labelCol=\"points\", featuresCol=\"features\")\n",
    "svm = LinearSVC(labelCol=\"points\", featuresCol=\"features\")\n",
    "rfr = RandomForestRegressor(labelCol= 'points', featuresCol = 'features', numTrees=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "l_pipeline = Pipeline(stages = [assembler,l])\n",
    "l_model = l_pipeline.fit(train)\n",
    "l_evaluator = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'points', metricName = 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_l = l_model.transform(test)\n",
    "r2_l = l_evaluator.evaluate(prediction_l)\n",
    "r2_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "lr_pipeline = Pipeline(stages = [assembler, lr])\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "lr_evaluator = MulticlassClassificationEvaluator(labelCol=\"points\",  metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lr = lr_model.transform(test)\n",
    "accuracy_lr = lr_evaluator.evaluate(prediction_lr)\n",
    "accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline(stages = [assembler,nb])\n",
    "nb_model = nb_pipeline.fit(train)\n",
    "evaluator_nb = MulticlassClassificationEvaluator(labelCol=\"points\",  metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_nb = nb_model.transform(test)\n",
    "accuracy_nb = evaluator_nb.evaluate(prediction_nb)\n",
    "accuracy_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(stages = [assembler, rfr])\n",
    "rf_model = rf_pipeline.fit(train)\n",
    "evaluator_rf = RegressionEvaluator(predictionCol = 'prediction', labelCol = 'points', metricName = 'r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rf = rf_model.transform(test)\n",
    "accuracy_rf = evaluator_rf.evaluate(prediction_rf)\n",
    "accuracy_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are selecting the linear regression and random forest resgressor as our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.3'></a>\n",
    "## 6.3 Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we run 4 models we are going to show the predictions for linear regression and random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_l.groupby('name').agg(sum('prediction').alias('predicted_points')).sort(desc('predicted_points')).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rf.groupby('name').agg(sum('prediction').alias('predicted_points')).sort(desc('predicted_points')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
